{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dynamic-finance",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dask_df\n",
    "import numpy as np\n",
    "import pyspark.sql.types as T\n",
    "import plotly.express as px\n",
    "import warnings\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import lag\n",
    "from pyspark.sql.functions import mean\n",
    "from pyspark.sql.functions import max\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.functions import round\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.functions import split\n",
    "from pyspark.sql import Window\n",
    "\n",
    "# suppress warning messages\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "third-signal",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function returns a Spark session\n",
    "\n",
    "def init_spark():\n",
    "    spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .appName('M5 Forecasting') \\\n",
    "        .config('spark.some.config.option', 'some-value') \\\n",
    "        .getOrCreate()\n",
    "    return spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "everyday-roads",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read melted sales data for all stores from the disk into a Spark dataframe\n",
    "\n",
    "spark = init_spark()\n",
    "\n",
    "sales_melted = spark.read.csv('./m5-forecasting-accuracy/sales_by_store/*', header = True). \\\n",
    "                     drop('_c0')\n",
    "sales_melted = sales_melted.withColumn('units_sold', sales_melted['units_sold'].cast('int'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "advanced-syria",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create mean features for various categorical features\n",
    "\n",
    "mean_by_item = sales_melted.groupBy('item_id').agg(round(mean('units_sold'), 4).alias('item_mean'))\n",
    "mean_by_dept = sales_melted.groupBy('dept_id').agg(round(mean('units_sold'), 4).alias('dept_mean'))\n",
    "mean_by_cat = sales_melted.groupBy('cat_id').agg(round(mean('units_sold'), 4).alias('cat_mean'))\n",
    "mean_by_store = sales_melted.groupBy('store_id').agg(round(mean('units_sold'), 4).alias('store_mean'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "wireless-wisconsin",
   "metadata": {},
   "outputs": [],
   "source": [
    "# since fitting a model to the entire data at once is a very costly operation that will not be feasible on a personal computer,\n",
    "# we will fit a model to an individual store\n",
    "# we can fit different models for different stores just by changing the selection below\n",
    "\n",
    "# select a store to apply feature engineering and machine learning model fitting on\n",
    "\n",
    "current_store = 'CA_1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "color-evolution",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read melted sales data for the specified store, calendar data, and prices data into Spark dataframes\n",
    "# merge the three dataframes\n",
    "# obtaining day numbers from the 'd' column, e.g. taking out 41 from d_41\n",
    "# changing columns' datatypes appropriately\n",
    "# replacing null events' values with 'NoEvent'\n",
    "\n",
    "spark = init_spark()\n",
    "\n",
    "sales_melted = spark.read.csv('./m5-forecasting-accuracy/sales_by_store/' + current_store + '/*', header = True). \\\n",
    "                     drop('_c0')\n",
    "sales_melted = sales_melted.where(sales_melted['d'].isin(['d_'+str(i) for i in np.arange(676, 1942)]))\n",
    "\n",
    "calendar = spark.read.csv('./m5-forecasting-accuracy/calendar.csv', header = True)\n",
    "prices = spark.read.csv('./m5-forecasting-accuracy/sell_prices.csv', header = True)\n",
    "\n",
    "sales_details = sales_melted.join(calendar, 'd', 'left_outer'). \\\n",
    "                             join(prices, ['store_id', 'item_id', 'wm_yr_wk'], 'left_outer')\n",
    "\n",
    "sales_details = sales_details.select(split(sales_details.d, '_')[1].alias('day'), \\\n",
    "                                     'item_id', 'dept_id', 'store_id', 'cat_id', 'date', 'wday', 'month', \\\n",
    "                                     'year', 'event_name_1', 'event_type_1', 'event_name_2', 'event_type_2', 'snap_CA', \n",
    "                                     'snap_TX', 'snap_WI', 'sell_price', 'units_sold')\n",
    "\n",
    "sales_details = sales_details.withColumn('day', sales_details['day'].cast('int')). \\\n",
    "                              withColumn('wday', sales_details['wday'].cast('int')). \\\n",
    "                              withColumn('month', sales_details['month'].cast('int')). \\\n",
    "                              withColumn('year', sales_details['year'].cast('int')). \\\n",
    "                              withColumn('snap_CA', sales_details['snap_CA'].cast('int')). \\\n",
    "                              withColumn('snap_TX', sales_details['snap_TX'].cast('int')). \\\n",
    "                              withColumn('snap_WI', sales_details['snap_WI'].cast('int')). \\\n",
    "                              withColumn('units_sold', sales_details['units_sold'].cast('int')). \\\n",
    "                              withColumn('sell_price', sales_details['sell_price'].cast('float'))\n",
    "\n",
    "sales_details = sales_details.fillna('NoEvent', subset = ['event_name_1', 'event_type_1', 'event_name_2', 'event_type_2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "present-attempt",
   "metadata": {},
   "outputs": [],
   "source": [
    "# introducing lags to create lag features\n",
    "# since we have introduced lags of 7, 14, and 21 days too, we will need to make forecasts recursively over periods of 7 days\n",
    "# that is, in order to make forecast for days 8 to 14, we will first need a forecast for days 1 to 7\n",
    "# using these forecasted units sold of days 1 to 7, we will recalculate features for days 8 to 14\n",
    "# then we can make a forecast for days 8 to 14\n",
    "\n",
    "lags = [7, 14, 21, 28, 35, 42, 49]\n",
    "window = Window.partitionBy(['item_id']).orderBy('day')\n",
    "\n",
    "for lag_duration in lags:\n",
    "    sales_details = sales_details.withColumn('sold_lag_' + str(lag_duration), \\\n",
    "                                             lag(col = sales_details['units_sold'], offset = lag_duration).over(window))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "sunrise-benchmark",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding the mean features we calculated beforeheand to our data\n",
    "\n",
    "sales_details = sales_details.join(mean_by_item, 'item_id', 'left_outer'). \\\n",
    "                              join(mean_by_dept, 'dept_id', 'left_outer'). \\\n",
    "                              join(mean_by_cat, 'cat_id', 'left_outer'). \\\n",
    "                              join(mean_by_store, 'store_id', 'left_outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "practical-treatment",
   "metadata": {},
   "outputs": [],
   "source": [
    "# introducing rolling mean features\n",
    "# we calculate mean over windows of sizes 7, 14, 28, 56, and 112\n",
    "# we have kept the lag to 28 days, so we won't need a recursive feature calculation for these features\n",
    "\n",
    "window = Window.partitionBy(['item_id']).orderBy('day')\n",
    "\n",
    "window_sizes = np.array([7, 14, 28, 56, 112])\n",
    "stop = -28\n",
    "start_values = stop - window_sizes + 1\n",
    "\n",
    "for index, start in enumerate(start_values.tolist()):\n",
    "    col_name = 'sold_rolling_mean_window_' + str(window_sizes[index]) + '_lag_' + str(abs(stop))\n",
    "    sales_details = sales_details.withColumn(col_name, \\\n",
    "                                  mean(col = sales_details['units_sold']).over(window.rangeBetween(start, stop)).cast('float'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "unique-genius",
   "metadata": {},
   "outputs": [],
   "source": [
    "# introducing rolling mean features\n",
    "# we calculate mean over windows of size 7\n",
    "# we have kept the lag to 7 and 365 days\n",
    "# we will need recursive feature calculation for the feature with a lag of 7 days\n",
    "\n",
    "window = Window.partitionBy(['item_id']).orderBy('day')\n",
    "\n",
    "window_size = 7\n",
    "stop_values = np.array([-7, -365])\n",
    "\n",
    "for stop in stop_values.tolist():\n",
    "    col_name = 'sold_rolling_mean_window_' + str(window_size) + '_lag_' + str(abs(stop))\n",
    "    sales_details = sales_details.withColumn(col_name, \\\n",
    "                                             mean(col = sales_details['units_sold']). \\\n",
    "                                             over(window.rangeBetween(stop - window_size + 1, stop)).cast('float'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "premium-elite",
   "metadata": {},
   "outputs": [],
   "source": [
    "# introducing expanding mean feature\n",
    "# we have kept the lag to 28 days, so we won't need a recursive feature calculation for this feature\n",
    "\n",
    "stop_values = np.array([-7, -365])\n",
    "\n",
    "for stop in stop_values.tolist():\n",
    "    window = Window.partitionBy(['item_id']).orderBy('day').rangeBetween(Window.unboundedPreceding, stop)\n",
    "\n",
    "    sales_details = sales_details.withColumn('sold_expanding_mean_lag_' + str(abs(stop)), \\\n",
    "                                             mean(col = sales_details['units_sold']).over(window).cast('float'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "integrated-racing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# introducing rolling mean features for price\n",
    "# we calculate mean over a window of size 7, and max over a window of size 365\n",
    "# we have kept the lag to 1\n",
    "# we won't need recursive feature calculation since we assume that we will have price details available in advance\n",
    "\n",
    "window = Window.partitionBy(['item_id']).orderBy('day')\n",
    "\n",
    "window_size = 7\n",
    "stop = -1\n",
    "start = stop - window_size + 1\n",
    "\n",
    "col_name = 'price_rolling_mean_window_' + str(window_size) + '_lag_' + str(abs(stop))\n",
    "sales_details = sales_details.withColumn(col_name, \\\n",
    "                              mean(col = sales_details['sell_price']).over(window.rangeBetween(start, stop)).cast('float'))\n",
    "\n",
    "window_size = 365\n",
    "stop = -1\n",
    "start = stop - window_size + 1\n",
    "\n",
    "col_name = 'price_rolling_max_window_' + str(window_size) + '_lag_' + str(abs(stop))\n",
    "sales_details = sales_details.withColumn(col_name, \\\n",
    "                              max(col = sales_details['sell_price']).over(window.rangeBetween(start, stop)).cast('float'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "approximate-cradle",
   "metadata": {},
   "outputs": [],
   "source": [
    "# introducing revenue and trend features\n",
    "# we won't need recursive feature calculation for revenue feature but we will need it for variance trend feature\n",
    "\n",
    "window = Window.partitionBy(['item_id']).orderBy('day')\n",
    "\n",
    "sales_details = sales_details.withColumn('sold_revenue_lag_28', \\\n",
    "                                         lag(col = (sales_details['units_sold'] * sales_details['sell_price']), \\\n",
    "                                         offset = 28).over(window))\n",
    "\n",
    "sales_details = sales_details.withColumn('variance_trend_lag_7', \\\n",
    "                                         sales_details['sold_rolling_mean_window_7_lag_7'] - sales_details['item_mean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "exact-algorithm",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to reduce the size of the dataset to be able to run further steps in vanilla python environment,\n",
    "# we take the data from 2013 to 2016\n",
    "# also more recent data should be more relevant for the predictions\n",
    "\n",
    "sales_details = sales_details.where(sales_details['day'] >= 1041)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "humanitarian-difficulty",
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting the dataset into train and test in such a way that test set consists of data for the last 28 days and train set\n",
    "# consists of the entire remaining data\n",
    "# saving this data in files to load it with ease in pandas, and also to avoid recalculation every time\n",
    " \n",
    "train = sales_details[sales_details['day'] < 1914]\n",
    "test = sales_details[sales_details['day'] >= 1914]\n",
    "\n",
    "sales_details.write.csv('./m5-forecasting-accuracy/train_test_split/sales_details/', header = True)\n",
    "train.write.csv('./m5-forecasting-accuracy/train_test_split/train/', header = True)\n",
    "test.write.csv('./m5-forecasting-accuracy/train_test_split/test/', header = True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
